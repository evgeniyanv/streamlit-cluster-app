import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN
from sklearn.ensemble import RandomForestClassifier
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.metrics import silhouette_score, davies_bouldin_score

from preprocessing import (
    clean_dataset,
    remove_outliers_iqr,
    detect_column_types,
    add_standard_features_v2,
    prepare_features_with_standard
)

# –ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å FAISS
try:
    import faiss
    faiss_available = True
except ImportError:
    faiss_available = False


# ---------- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ----------
st.title("üîé –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–∞—Ä–æ–∫ —Å—Ç–∞–ª–∏")
st.markdown("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –æ—á–∏—Å—Ç–∫—É, –≤—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")

# --- –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ XLSX", type=["csv", "xlsx"])

if uploaded_file is not None:
    if uploaded_file.name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_excel(uploaded_file)

    st.write("üìä –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    st.dataframe(df.head())

    before_rows = len(df)

    # --- –ß–µ–∫–±–æ–∫—Å—ã –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
    remove_dupes = st.checkbox("–£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫", value=True)
    remove_nans = st.checkbox("–£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏", value=True)
    remove_outliers = st.checkbox("–£–¥–∞–ª–∏—Ç—å –≤—ã–±—Ä–æ—Å—ã –ø–æ IQR", value=False)

    if remove_dupes:
        df = df.drop_duplicates()
    if remove_nans:
        df = clean_dataset(df)
    if remove_outliers:
        numeric_cols = df.select_dtypes(include="number").columns.tolist()
        df = remove_outliers_iqr(df, numeric_cols)

    after_rows = len(df)
    st.success(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ë—ã–ª–æ: {before_rows}, —Å—Ç–∞–ª–æ: {after_rows}")

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ ST_PROD –î–û –≤—ã–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if "ST_PROD" in df.columns:
        df = add_standard_features_v2(df, col="ST_PROD")
    else:
        st.info("‚ÑπÔ∏è –ö–æ–ª–æ–Ω–∫–∞ 'ST_PROD' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –ø—Ä–∏–∑–Ω–∞–∫–∏ system, code, year, has_updates –Ω–µ –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")

    # --- –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏
    all_cols = list(df.columns)

    # --- –ò—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü ST_PROD, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    exclude_cols = ["ST_PROD"]  

    # --- –í—ã–±–æ—Ä —Ç–∞—Ä–≥–µ—Ç–∞
    target_col = st.selectbox("üéØ –í—ã–±–µ—Ä–∏—Ç–µ —Ç–∞—Ä–≥–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", [None] + all_cols)
    if target_col:
        exclude_cols.append(target_col)

    # --- –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    available_cols = [c for c in all_cols if c not in exclude_cols]

    # --- –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    selected_cols = st.multiselect(
        "üß© –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
        available_cols,
        default=available_cols
    )

    st.info(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(selected_cols)}")

    # --- –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
    if selected_cols:
        df_checked, numeric_cols, categorical_cols = detect_column_types(df[selected_cols].copy())
        st.subheader("üîé –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        st.markdown(f"**–ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(numeric_cols)}):** {', '.join(numeric_cols) if numeric_cols else '–Ω–µ—Ç'}")
        st.markdown(f"**–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(categorical_cols)}):** {', '.join(categorical_cols) if categorical_cols else '–Ω–µ—Ç'}")

    # --- –°–ª—É–∂–µ–±–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –±–ª–æ–∫–µ)
    with st.expander("üîß –°–ª—É–∂–µ–±–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)"):
        service_cols = [c for c in ["system", "code", "year", "has_updates"] if c in df.columns]
        if service_cols:
            st.dataframe(df[service_cols].head())
        else:
            st.write("–ù–µ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")

    # --- –ê–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    
    n_rows = len(df)

    algo_options = []
    algo_info = []

    # KMeans –≤—Å–µ–≥–¥–∞
    algo_options.append("KMeans (–¥–æ ~100k —Å—Ç—Ä–æ–∫)")

    # Agglomerative
    if n_rows <= 20000:
        algo_options.append("Agglomerative (–¥–æ ~10k —Å—Ç—Ä–æ–∫, –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π)")
    else:
        algo_info.append("‚ùå Agglomerative –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ >20k —Å—Ç—Ä–æ–∫ (—Å–ª–∏—à–∫–æ–º —Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–∏–π).")

    # DBSCAN
    if n_rows <= 50000:
        algo_options.append("DBSCAN (–¥–æ ~50k —Å—Ç—Ä–æ–∫, –ø–ª–æ—Ç–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã)")
    else:
        algo_info.append("‚ùå DBSCAN –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ >50k —Å—Ç—Ä–æ–∫ (–º–µ–¥–ª–µ–Ω–Ω–æ –∏ –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏).")

    # FAISS
    if faiss_available:
        algo_options.append("FAISS HNSW (–º–∏–ª–ª–∏–æ–Ω—ã —Å—Ç—Ä–æ–∫, –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)")
    else:
        algo_info.append("‚ùå FAISS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç `faiss-cpu`.")

    # –î–æ–ø. –∏–Ω—Ñ–æ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    if n_rows > 100000:
        algo_info.append("‚ö†Ô∏è –î–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ >100k —Å—Ç—Ä–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PCA –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–º–µ—Å—Ç–æ UMAP/t-SNE.")

    # Selectbox
    algo = st.selectbox("ü§ñ –ê–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", algo_options)

    # –ü–æ—è—Å–Ω–µ–Ω–∏—è
    if algo_info:
        st.markdown("### ‚ÑπÔ∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –≤—ã–±–æ—Ä—É –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤")
        for msg in algo_info:
            st.markdown(msg)

    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    n_clusters, eps, min_samples, M = None, None, None, None
    if "KMeans" in algo:
        n_clusters = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (KMeans)", 2, 20, 5)
    elif "Agglomerative" in algo:
        n_clusters = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (Agglomerative)", 2, 15, 5)
    elif "DBSCAN" in algo:
        eps = st.slider("eps (—Ä–∞–¥–∏—É—Å —Å–æ—Å–µ–¥—Å—Ç–≤–∞, DBSCAN)", 0.1, 10.0, 0.5, step=0.1)
        min_samples = st.slider("min_samples (–º–∏–Ω–∏–º—É–º —Ç–æ—á–µ–∫ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ, DBSCAN)", 2, 20, 5)
    elif "FAISS" in algo:
        n_clusters = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (FAISS HNSW)", 2, 100, 10)
        M = st.slider("M (—Ä–∞–∑–º–µ—Ä –≥—Ä–∞—Ñ–∞ HNSW)", 8, 64, 32)

    # --- –ó–∞–ø—É—Å–∫
    if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é"):
        X, y, filled_cols, log_messages, feature_names = prepare_features_with_standard(df, selected_cols, target_col)
        # –í—ã–≤–æ–¥–∏–º –ª–æ–≥ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        with st.expander("üìã –õ–æ–≥ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"):
            for msg in log_messages:
                st.markdown(msg)

        if filled_cols:
            st.warning(f"‚ö† –ó–∞–ø–æ–ª–Ω–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –≤: {', '.join(filled_cols)}")

        if X.shape[0] == 0:
            st.error("–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏!")
        else:
            if "KMeans" in algo:
                model = KMeans(n_clusters=n_clusters, random_state=42, n_init="auto")
                clusters = model.fit_predict(X)
            elif "Agglomerative" in algo:
                model = AgglomerativeClustering(n_clusters=n_clusters, linkage="ward")
                clusters = model.fit_predict(X.toarray() if hasattr(X, "toarray") else X)
            elif "DBSCAN" in algo:
                model = DBSCAN(eps=eps, min_samples=min_samples)
                clusters = model.fit_predict(X.toarray() if hasattr(X, "toarray") else X)
            elif "FAISS" in algo:
                if not faiss_available:
                    st.error("‚ö† FAISS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install faiss-cpu")
                    clusters = np.zeros(X.shape[0], dtype=int)
                else:
                    X_f32 = X.astype(np.float32).toarray() if hasattr(X, "toarray") else X.astype(np.float32)
                    d = X_f32.shape[1]
                    index = faiss.IndexHNSWFlat(d, M)
                    index.hnsw.efConstruction, index.hnsw.efSearch = 200, 50
                    index.add(X_f32)
                    kmeans = faiss.Kmeans(d, n_clusters, niter=20, verbose=False)
                    kmeans.train(X_f32)
                    _, clusters = kmeans.index.search(X_f32, 1)
                    clusters = clusters.flatten()

            df["cluster"] = clusters

            # --- –ú–µ—Ç—Ä–∏–∫–∏
            st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
            valid_clusters = set(clusters)
            if len(valid_clusters) > 1 and not (-1 in valid_clusters and "DBSCAN" in algo):
                sample_size = min(5000, X.shape[0])
                if X.shape[0] > sample_size:
                    idx = np.random.choice(X.shape[0], sample_size, replace=False)
                    X_eval, clusters_eval = X[idx], clusters[idx]
                else:
                    X_eval, clusters_eval = X, clusters

                sil_score = silhouette_score(X_eval, clusters_eval)
                db_score = davies_bouldin_score(X_eval, clusters_eval)

                st.success(f"Silhouette Score: **{sil_score:.3f}** (–±–ª–∏–∂–µ –∫ 1 –ª—É—á—à–µ)")
                st.info(f"Davies‚ÄìBouldin Index: **{db_score:.3f}** (–±–ª–∏–∂–µ –∫ 0 –ª—É—á—à–µ)")
            else:
                st.warning("‚ö† –ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã (—Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–ª–∏ –µ—Å—Ç—å —à—É–º–æ–≤—ã–µ –º–µ—Ç–∫–∏).")

            # --- –ë–∞—Ä—á–∞—Ä—Ç
            st.subheader("üìä –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            cluster_sizes = df["cluster"].value_counts().reset_index()
            cluster_sizes.columns = ["cluster", "size"]
            fig_bar = px.bar(cluster_sizes, x="cluster", y="size", text="size",
                            title="–†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", color="cluster")
            st.plotly_chart(fig_bar, use_container_width=True)

            # --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ---
            st.subheader("üåê –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            max_viz_samples = 5000
            if len(X) > max_viz_samples:
                viz_idx = np.random.choice(len(X), max_viz_samples, replace=False)
                X_viz = X[viz_idx]
                clusters_viz = clusters[viz_idx]
                df_viz = df.iloc[viz_idx].copy()
                st.info(f"‚ö†Ô∏è –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ –∏–∑ {max_viz_samples} —Å—Ç—Ä–æ–∫ (–≤—Å–µ–≥–æ {len(X)}).")
            else:
                X_viz = X
                clusters_viz = clusters
                df_viz = df.copy()

            # --- –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 2D
            try:
                if len(X_viz) <= 5000:
                    reducer_2d = TSNE(n_components=2, random_state=42, perplexity=30)
                    X_embedded_2d = reducer_2d.fit_transform(X_viz.toarray() if hasattr(X_viz, "toarray") else X_viz)
                    method_2d = "t-SNE"
                else:
                    reducer_2d = umap.UMAP(n_neighbors=30, min_dist=0.1, random_state=42, low_memory=True, n_components=2)
                    X_embedded_2d = reducer_2d.fit_transform(X_viz.toarray() if hasattr(X_viz, "toarray") else X_viz)
                    method_2d = "UMAP"
            except Exception:
                from sklearn.decomposition import PCA
                reducer_2d = PCA(n_components=2, random_state=42)
                X_embedded_2d = reducer_2d.fit_transform(X_viz.toarray() if hasattr(X_viz, "toarray") else X_viz)
                method_2d = "PCA"

            viz_df_2d = pd.DataFrame({
                "x": X_embedded_2d[:, 0],
                "y": X_embedded_2d[:, 1],
                "cluster": clusters_viz.astype(int)
            })


            viz_df_2d["cluster"] = viz_df_2d["cluster"].astype(str)

            # –î–æ–±–∞–≤–∏–º –ø–µ—Ä–≤—ã–µ 3 –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏ –Ω–∞–≤–µ–¥–µ–Ω–∏–∏
            for col in selected_cols[:3]:
                if col in df.columns:
                    viz_df_2d[col] = df[col].iloc[:len(viz_df_2d)].values

            fig2d = px.scatter(
                viz_df_2d,
                x="x", y="y",
                color="cluster",
                hover_data=selected_cols[:3],
                color_discrete_sequence=px.colors.qualitative.Vivid,
                title=f"2D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ({method_2d}, {algo})"
            )

            st.plotly_chart(fig2d, use_container_width=True)


            # --- –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 3D (t-SNE / UMAP / PCA –ø–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞)
            try:
                if len(X_viz) <= 10000:
                    reducer_3d = TSNE(n_components=3, random_state=42, perplexity=30)
                    X_embedded_3d = reducer_3d.fit_transform(X_viz.toarray() if hasattr(X_viz, "toarray") else X_viz)
                    method_3d = "t-SNE 3D"
                elif len(X_viz) <= 150000:
                    reducer_3d = umap.UMAP(n_neighbors=30, min_dist=0.1, random_state=42, low_memory=True, n_components=3)
                    X_embedded_3d = reducer_3d.fit_transform(X_viz.toarray() if hasattr(X_viz, "toarray") else X_viz)
                    method_3d = "UMAP 3D"
                else:
                    reducer_3d = PCA(n_components=3, random_state=42)
                    X_embedded_3d = reducer_3d.fit_transform(X_viz.toarray() if hasattr(X_viz, "toarray") else X_viz)
                    method_3d = "PCA 3D"
            except Exception:
                reducer_3d = PCA(n_components=3, random_state=42)
                X_embedded_3d = reducer_3d.fit_transform(X_viz.toarray() if hasattr(X_viz, "toarray") else X_viz)
                method_3d = "PCA 3D (fallback)"

            viz_df_3d = pd.DataFrame({
                "x": X_embedded_3d[:, 0],
                "y": X_embedded_3d[:, 1],
                "z": X_embedded_3d[:, 2],
                "cluster": clusters_viz.astype(int)
            })

            fig3d = go.Figure(data=go.Scatter3d(
                x=viz_df_3d["x"], y=viz_df_3d["y"], z=viz_df_3d["z"],
                mode="markers",
                marker=dict(
                    color=viz_df_3d["cluster"],
                    colorscale="Rainbow",
                    size=2,
                    opacity=0.7,
                    colorbar=dict(title="Cluster")
                ),
                text=[f"Cluster: {c}" for c in viz_df_3d["cluster"]]
            ))
            fig3d.update_layout(title=f"3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ({method_3d}, {algo})")
            st.plotly_chart(fig3d, use_container_width=True)

            # --- –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è Agglomerative

            def plot_dendrogram(X, method="ward", max_samples=500):
                # –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –æ—á–µ–Ω—å –º–Ω–æ–≥–æ, –±–µ—Ä—ë–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É
                if X.shape[0] > max_samples:
                    idx = np.random.choice(X.shape[0], max_samples, replace=False)
                    X_sample = X[idx]
                else:
                    X_sample = X

                # linkage matrix
                Z = linkage(X_sample.toarray() if hasattr(X_sample, "toarray") else X_sample, method=method)

                # —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫
                fig, ax = plt.subplots(figsize=(12, 6))
                dendrogram(Z, ax=ax, truncate_mode="level", p=10)  # p=10 ‚Üí –≥–ª—É–±–∏–Ω–∞ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö —É—Ä–æ–≤–Ω–µ–π
                ax.set_title("–î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞ (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è)")
                ax.set_xlabel("–û–±—ä–µ–∫—Ç—ã –∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä—ã")
                ax.set_ylabel("–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ")
                st.pyplot(fig)

            if "Agglomerative" in algo:
                st.subheader("üå≥ –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è Agglomerative Clustering")
                plot_dendrogram(X, method="ward")

 
            # --- –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ---

            st.subheader("üß© –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

            try:
                X_rf = X
                y_rf = df["cluster"].astype(str)

                if hasattr(X_rf, "toarray"):
                    X_rf = X_rf.toarray()

                rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
                rf.fit(X_rf, y_rf)

                # —Å–æ–∑–¥–∞—ë–º DataFrame —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                importances_df = pd.DataFrame({
                    "–ü—Ä–∏–∑–Ω–∞–∫": feature_names,
                    "–í–∞–∂–Ω–æ—Å—Ç—å": rf.feature_importances_
                }).sort_values(by="–í–∞–∂–Ω–æ—Å—Ç—å", ascending=False)

                top_features = importances_df.head(5)

                st.success("‚úÖ Random Forest –æ–±—É—á–µ–Ω. –ù–∏–∂–µ ‚Äî 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–∞–∏–±–æ–ª–µ–µ –≤–ª–∏—è—é—â–∏—Ö –Ω–∞ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã.")
                st.table(top_features.round(4))

                fig_importance = px.bar(
                    top_features[::-1],
                    x="–í–∞–∂–Ω–æ—Å—Ç—å", y="–ü—Ä–∏–∑–Ω–∞–∫",
                    orientation="h",
                    title="–¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (Random Forest)",
                    color="–í–∞–∂–Ω–æ—Å—Ç—å",
                    color_continuous_scale="Viridis"
                )
                st.plotly_chart(fig_importance, use_container_width=True)

            except Exception as e:
                st.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")


            # --- –¢–∞–±–ª–∏—Ü–∞
            st.subheader("üìã –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            st.dataframe(df[selected_cols + ["cluster"]].head(10))
            
            # --- –ö–Ω–æ–ø–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
            st.subheader("üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")

            # CSV ‚Äî –≤—Å–µ–≥–¥–∞
            csv_data = df.to_csv(index=False).encode("utf-8")
            st.download_button(
                label="–°–∫–∞—á–∞—Ç—å CSV",
                data=csv_data,
                file_name="clusters.csv",
                mime="text/csv"
            )

            # Excel ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            if len(df) <= 100000:
                excel_file = "clusters.xlsx"
                df.to_excel(excel_file, index=False)
                with open(excel_file, "rb") as f:
                    st.download_button(
                        label="–°–∫–∞—á–∞—Ç—å Excel",
                        data=f,
                        file_name="clusters.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
            else:
                st.info("‚ÑπÔ∏è Excel-—Ñ–∞–π–ª –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ >100k —Å—Ç—Ä–æ–∫, "
                        "—Ç–∞–∫ –∫–∞–∫ –æ–Ω –±—É–¥–µ—Ç —Å–ª–∏—à–∫–æ–º —Ç—è–∂—ë–ª—ã–º. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CSV.")
